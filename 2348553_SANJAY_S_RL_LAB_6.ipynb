{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh1YpXT/r1EDyYCtZwxx+q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayS2348553/Reinforcement-Learning/blob/main/2348553_SANJAY_S_RL_LAB_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e70HnG4_n4wa",
        "outputId": "3f7d6a3e-c610-4300-c9b7-c3268b36ca38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install gymnasium numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class GridWorldEnv(gym.Env):\n",
        "    def __init__(self, grid_size=5, target_position=(4, 4), obstacle_positions=[]):\n",
        "        super(GridWorldEnv, self).__init__()\n",
        "\n",
        "        # Define grid size\n",
        "        self.grid_size = grid_size\n",
        "        self.target_position = target_position\n",
        "        self.obstacle_positions = set(obstacle_positions)\n",
        "\n",
        "        # Action space: 0 = up, 1 = right, 2 = down, 3 = left\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        # Observation space: agent's position in the grid\n",
        "        self.observation_space = spaces.Box(low=0, high=grid_size - 1, shape=(2,), dtype=np.int32)\n",
        "\n",
        "        # Initialize agent's position\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_position = np.array([0, 0])  # Starting position\n",
        "        return self.agent_position\n",
        "\n",
        "    def step(self, action):\n",
        "        # Move agent\n",
        "        if action == 0:  # Up\n",
        "            self.agent_position[0] = max(0, self.agent_position[0] - 1)\n",
        "        elif action == 1:  # Right\n",
        "            self.agent_position[1] = min(self.grid_size - 1, self.agent_position[1] + 1)\n",
        "        elif action == 2:  # Down\n",
        "            self.agent_position[0] = min(self.grid_size - 1, self.agent_position[0] + 1)\n",
        "        elif action == 3:  # Left\n",
        "            self.agent_position[1] = max(0, self.agent_position[1] - 1)\n",
        "\n",
        "        # Check if agent hits an obstacle\n",
        "        if tuple(self.agent_position) in self.obstacle_positions:\n",
        "            reward = -1\n",
        "            done = False\n",
        "        # Check if agent reaches the target\n",
        "        elif np.array_equal(self.agent_position, self.target_position):\n",
        "            reward = 10\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -0.1\n",
        "            done = False\n",
        "\n",
        "        return self.agent_position, reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
        "        grid[:] = '.'\n",
        "        grid[tuple(self.target_position)] = 'T'\n",
        "        for obs in self.obstacle_positions:\n",
        "            grid[obs] = 'X'\n",
        "        grid[tuple(self.agent_position)] = 'A'\n",
        "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
        "        print()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridWorldEnv(obstacle_positions=[(2, 2), (3, 3)])\n",
        "    obs = env.reset()\n",
        "    env.render()\n",
        "\n",
        "    for _ in range(10):\n",
        "        action = env.action_space.sample()\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        env.render()\n",
        "        if done:\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8HAOg6zsz5s",
        "outputId": "e3b2d827-be84-4b43-c290-659af8732427"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A . . . .\n",
            ". . . . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . T\n",
            "\n",
            ". . . . .\n",
            "A . . . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . T\n",
            "\n",
            ". . . . .\n",
            ". A . . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . T\n",
            "\n",
            ". A . . .\n",
            ". . . . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . T\n",
            "\n",
            "A . . . .\n",
            ". . . . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . T\n",
            "\n",
            ". A . . .\n",
            ". . . . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . T\n",
            "\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . T\n",
            "\n",
            ". . . . .\n",
            ". . A . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . T\n",
            "\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . T\n",
            "\n",
            ". A . . .\n",
            ". . . . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . T\n",
            "\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . T\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ivHoLhW4vDG8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "class ContinuousPendulumEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(ContinuousPendulumEnv, self).__init__()\n",
        "\n",
        "        # State: [theta (angle), theta_dot (angular velocity)]\n",
        "        self.observation_space = spaces.Box(low=np.array([-np.pi, -8]),\n",
        "                                            high=np.array([np.pi, 8]),\n",
        "                                            dtype=np.float32)\n",
        "\n",
        "        # Action: torque applied to the pendulum\n",
        "        self.action_space = spaces.Box(low=np.array([-2.0]),\n",
        "                                       high=np.array([2.0]),\n",
        "                                       dtype=np.float32)\n",
        "\n",
        "        # Simulation parameters\n",
        "        self.dt = 0.05\n",
        "        self.max_steps = 200\n",
        "        self.steps = 0\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Randomize initial state\n",
        "        self.state = np.array([np.random.uniform(-np.pi, np.pi), 0])  # [theta, theta_dot]\n",
        "        self.steps = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        theta, theta_dot = self.state\n",
        "        torque = np.clip(action, self.action_space.low, self.action_space.high)[0]\n",
        "\n",
        "        # Dynamics: d2(theta)/dt2 = -g/l * sin(theta) - b/m * d(theta)/dt + torque / (m * l^2)\n",
        "        g, l, m, b = 9.8, 1.0, 1.0, 0.1  # gravity, length, mass, damping coefficient\n",
        "        theta_ddot = (-g / l * np.sin(theta) - b / m * theta_dot + torque / (m * l**2))\n",
        "\n",
        "        # Update state using Euler integration\n",
        "        theta_dot += theta_ddot * self.dt\n",
        "        theta += theta_dot * self.dt\n",
        "\n",
        "        # Wrap theta to [-pi, pi]\n",
        "        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n",
        "\n",
        "        self.state = np.array([theta, theta_dot])\n",
        "        self.steps += 1\n",
        "\n",
        "        # Reward: stabilize the pendulum upright (theta = 0)\n",
        "        reward = - (theta**2 + 0.1 * theta_dot**2 + 0.01 * torque**2)\n",
        "        done = self.steps >= self.max_steps\n",
        "\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"State: {self.state}, Steps: {self.steps}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    env = ContinuousPendulumEnv()\n",
        "    state = env.reset()\n",
        "    for _ in range(100):\n",
        "        action = env.action_space.sample()  # Random action\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        env.render()\n",
        "        if done:\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns3K6q4KvC7C",
        "outputId": "0bdb046a-1aed-488f-f6e5-fa13d50b2a8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: [ 0.43078081 -0.28195749], Steps: 1\n",
            "State: [ 0.41020348 -0.41154659], Steps: 2\n",
            "State: [ 0.38162062 -0.57165721], Steps: 3\n",
            "State: [ 0.33910784 -0.85025558], Steps: 4\n",
            "State: [ 0.28582013 -1.06575423], Steps: 5\n",
            "State: [ 0.22384033 -1.23959607], Steps: 6\n",
            "State: [ 0.15784706 -1.31986535], Steps: 7\n",
            "State: [ 0.09185042 -1.31993285], Steps: 8\n",
            "State: [ 0.0243076  -1.35085627], Steps: 9\n",
            "State: [-0.04303934 -1.34693882], Steps: 10\n",
            "State: [-0.10798113 -1.29883583], Steps: 11\n",
            "State: [-0.17316731 -1.30372365], Steps: 12\n",
            "State: [-0.23452356 -1.2271249 ], Steps: 13\n",
            "State: [-0.28516072 -1.01274317], Steps: 14\n",
            "State: [-0.32627251 -0.82223591], Steps: 15\n",
            "State: [-0.35649959 -0.60454161], Steps: 16\n",
            "State: [-0.37921664 -0.45434107], Steps: 17\n",
            "State: [-0.39044353 -0.22453762], Steps: 18\n",
            "State: [-0.39171269 -0.02538329], Steps: 19\n",
            "State: [-0.3864791   0.10467181], Steps: 20\n",
            "State: [-0.3730814   0.26795397], Steps: 21\n",
            "State: [-0.35171478  0.42733243], Steps: 22\n",
            "State: [-0.32516364  0.53102273], Steps: 23\n",
            "State: [-0.28656601  0.77195264], Steps: 24\n",
            "State: [-0.24532754  0.82476942], Steps: 25\n",
            "State: [-0.19905886  0.92537368], Steps: 26\n",
            "State: [-0.14645894  1.05199831], Steps: 27\n",
            "State: [-0.08655573  1.19806422], Steps: 28\n",
            "State: [-0.02920475  1.1470196 ], Steps: 29\n",
            "State: [0.03141177 1.21233036], Steps: 30\n",
            "State: [0.09193461 1.21045691], Steps: 31\n",
            "State: [0.14549444 1.07119657], Steps: 32\n",
            "State: [0.19954207 1.08095256], Steps: 33\n",
            "State: [0.24907894 0.99073742], Steps: 34\n",
            "State: [0.28849727 0.7883666 ], Steps: 35\n",
            "State: [0.31972491 0.62455271], Steps: 36\n",
            "State: [0.34728473 0.55119646], Steps: 37\n",
            "State: [0.36366418 0.32758904], Steps: 38\n",
            "State: [0.36846565 0.09602929], Steps: 39\n",
            "State: [ 0.36356983 -0.09791623], Steps: 40\n",
            "State: [ 0.34845459 -0.30230492], Steps: 41\n",
            "State: [ 0.32132162 -0.5426594 ], Steps: 42\n",
            "State: [ 0.2889697  -0.64703841], Steps: 43\n",
            "State: [ 0.24579799 -0.86343423], Steps: 44\n",
            "State: [ 0.19721765 -0.97160662], Steps: 45\n",
            "State: [ 0.14664725 -1.01140812], Steps: 46\n",
            "State: [ 0.09682509 -0.99644309], Steps: 47\n",
            "State: [ 0.04767817 -0.98293844], Steps: 48\n",
            "State: [-0.00589317 -1.07142687], Steps: 49\n",
            "State: [-0.05878146 -1.05776588], Steps: 50\n",
            "State: [-0.11299434 -1.08425756], Steps: 51\n",
            "State: [-0.16239044 -0.98792191], Steps: 52\n",
            "State: [-0.20959184 -0.94402805], Steps: 53\n",
            "State: [-0.25358984 -0.87996007], Steps: 54\n",
            "State: [-0.28835998 -0.69540266], Steps: 55\n",
            "State: [-0.31541529 -0.54110623], Steps: 56\n",
            "State: [-0.33278956 -0.34748549], Steps: 57\n",
            "State: [-0.33815505 -0.10730964], Steps: 58\n",
            "State: [-0.33685099  0.02608118], Steps: 59\n",
            "State: [-0.32282464  0.28052688], Steps: 60\n",
            "State: [-0.29718458  0.51280133], Steps: 61\n",
            "State: [-0.26127206  0.71825029], Steps: 62\n",
            "State: [-0.21956262  0.83418872], Steps: 63\n",
            "State: [-0.17688916  0.85346922], Steps: 64\n",
            "State: [-0.13496343  0.8385146 ], Steps: 65\n",
            "State: [-0.09126234  0.87402179], Steps: 66\n",
            "State: [-0.04176546  0.98993768], Steps: 67\n",
            "State: [0.00416913 0.91869185], Steps: 68\n",
            "State: [0.048305   0.88271741], Steps: 69\n",
            "State: [0.0945914  0.92572793], Steps: 70\n",
            "State: [0.14084299 0.92503181], Steps: 71\n",
            "State: [0.18560266 0.89519331], Steps: 72\n",
            "State: [0.22482711 0.78448905], Steps: 73\n",
            "State: [0.25904713 0.6844005 ], Steps: 74\n",
            "State: [0.28738599 0.56677724], Steps: 75\n",
            "State: [0.30962699 0.44481999], Steps: 76\n",
            "State: [0.3292209  0.39187804], Steps: 77\n",
            "State: [0.34068071 0.22919637], Steps: 78\n",
            "State: [ 0.33915491 -0.03051606], Steps: 79\n",
            "State: [ 0.3342954  -0.09719015], Steps: 80\n",
            "State: [ 0.32612836 -0.16334091], Steps: 81\n",
            "State: [ 0.30974575 -0.32765212], Steps: 82\n",
            "State: [ 0.28157334 -0.5634483 ], Steps: 83\n",
            "State: [ 0.24560836 -0.71929945], Steps: 84\n",
            "State: [ 0.20177058 -0.87675578], Steps: 85\n",
            "State: [ 0.15755012 -0.88440911], Steps: 86\n",
            "State: [ 0.10891932 -0.97261596], Steps: 87\n",
            "State: [ 0.05989853 -0.98041589], Steps: 88\n",
            "State: [ 0.00822427 -1.03348524], Steps: 89\n",
            "State: [-0.03861358 -0.93675689], Steps: 90\n",
            "State: [-0.08362901 -0.90030853], Steps: 91\n",
            "State: [-0.12212808 -0.76998148], Steps: 92\n",
            "State: [-0.15812959 -0.72003021], Steps: 93\n",
            "State: [-0.18910699 -0.61954807], Steps: 94\n",
            "State: [-0.21528973 -0.52365473], Steps: 95\n",
            "State: [-0.23240641 -0.34233366], Steps: 96\n",
            "State: [-0.24096756 -0.17122298], Steps: 97\n",
            "State: [-0.24674738 -0.11559629], Steps: 98\n",
            "State: [-0.24975669 -0.06018624], Steps: 99\n",
            "State: [-0.2423702   0.14772986], Steps: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class StochasticBanditEnv(gym.Env):\n",
        "    def __init__(self, n_arms=5):\n",
        "        super(StochasticBanditEnv, self).__init__()\n",
        "\n",
        "        # Number of arms (actions)\n",
        "        self.n_arms = n_arms\n",
        "        self.action_space = spaces.Discrete(n_arms)\n",
        "\n",
        "        # Reward probabilities for each arm\n",
        "        self.reward_probs = np.random.uniform(0, 1, size=n_arms)\n",
        "\n",
        "    def reset(self):\n",
        "        # No state for bandits\n",
        "        return None\n",
        "\n",
        "    def step(self, action):\n",
        "        # Generate reward based on chosen arm's probability\n",
        "        assert self.action_space.contains(action), \"Invalid action\"\n",
        "        reward = np.random.rand() < self.reward_probs[action]\n",
        "        return None, float(reward), False, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"Reward probabilities: {self.reward_probs}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    env = StochasticBanditEnv()\n",
        "    env.render()\n",
        "    env.reset()\n",
        "    for _ in range(10):\n",
        "        action = env.action_space.sample()  # Random arm selection\n",
        "        _, reward, _, _ = env.step(action)\n",
        "        print(f\"Action: {action}, Reward: {reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7yxwl4PvD1a",
        "outputId": "1b4ababd-41ec-4c5b-c9c3-d8dc6bd9e5cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward probabilities: [0.99811657 0.790234   0.9300397  0.80288682 0.98474511]\n",
            "Action: 1, Reward: 0.0\n",
            "Action: 0, Reward: 1.0\n",
            "Action: 3, Reward: 1.0\n",
            "Action: 4, Reward: 1.0\n",
            "Action: 1, Reward: 1.0\n",
            "Action: 2, Reward: 1.0\n",
            "Action: 1, Reward: 1.0\n",
            "Action: 4, Reward: 1.0\n",
            "Action: 3, Reward: 1.0\n",
            "Action: 1, Reward: 1.0\n"
          ]
        }
      ]
    }
  ]
}