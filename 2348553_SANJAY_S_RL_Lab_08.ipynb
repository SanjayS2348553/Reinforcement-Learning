{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayS2348553/Reinforcement-Learning/blob/main/2348553_SANJAY_S_RL_Lab_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jxUbipvBGzc",
        "outputId": "295ae01a-78c2-4591-b569-9836abe582ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Value Function:\n",
            "State 1: 0.00\n",
            "State 2: -0.72\n",
            "State 3: -0.25\n",
            "State 4: 0.42\n",
            "State 5: 0.00\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Environment setup\n",
        "class GridWorld:\n",
        "    def __init__(self, n_states=5):\n",
        "        self.n_states = n_states\n",
        "        self.state = 1  # Start state\n",
        "        self.terminal_states = [1, n_states]\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = random.randint(2, self.n_states - 1)  # Start from a random non-terminal state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Action: -1 (left), +1 (right)\n",
        "        Returns: next_state, reward, done\n",
        "        \"\"\"\n",
        "        next_state = self.state + action\n",
        "        if next_state <= 1:\n",
        "            next_state = 1\n",
        "            reward = -1  # Penalty for hitting left terminal state\n",
        "            done = True\n",
        "        elif next_state >= self.n_states:\n",
        "            next_state = self.n_states\n",
        "            reward = 1  # Reward for hitting right terminal state\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0  # No reward for intermediate states\n",
        "            done = False\n",
        "\n",
        "        self.state = next_state\n",
        "        return next_state, reward, done\n",
        "\n",
        "\n",
        "# TD(0) Algorithm\n",
        "def td_zero(env, gamma=0.9, alpha=0.1, episodes=100):\n",
        "    V = np.zeros(env.n_states + 1)  # State value function (1-based indexing)\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = random.choice([-1, 1])  # Random policy\n",
        "            next_state, reward, done = env.step(action)\n",
        "            # TD(0) update rule\n",
        "            V[state] += alpha * (reward + gamma * V[next_state] - V[state])\n",
        "            state = next_state\n",
        "\n",
        "    return V\n",
        "\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridWorld(n_states=5)\n",
        "    episodes = 500\n",
        "    alpha = 0.1\n",
        "    gamma = 0.9\n",
        "\n",
        "    value_function = td_zero(env, gamma=gamma, alpha=alpha, episodes=episodes)\n",
        "\n",
        "    print(\"State Value Function:\")\n",
        "    for state in range(1, env.n_states + 1):\n",
        "        print(f\"State {state}: {value_function[state]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment:\n",
        "\n",
        "The agent operates in a 1D grid world with 5 states numbered from 1 to 5.\n",
        "States 1 and 5 are terminal states with rewards -1 and +1, respectively.\n",
        "Actions move the agent left (-1) or right (+1). Intermediate states provide a reward of 0.\n",
        "TD(0) Update Rule:\n",
        "\n",
        "The state-value function\n",
        "ğ‘‰\n",
        "(\n",
        "ğ‘ \n",
        ")\n",
        "V(s) is updated as follows:\n",
        "ğ‘‰\n",
        "(\n",
        "ğ‘ \n",
        ")\n",
        "â†\n",
        "ğ‘‰\n",
        "(\n",
        "ğ‘ \n",
        ")\n",
        "+\n",
        "ğ›¼\n",
        "[\n",
        "ğ‘…\n",
        "+\n",
        "ğ›¾\n",
        "ğ‘‰\n",
        "(\n",
        "ğ‘ \n",
        "â€²\n",
        ")\n",
        "âˆ’\n",
        "ğ‘‰\n",
        "(\n",
        "ğ‘ \n",
        ")\n",
        "]\n",
        "V(s)â†V(s)+Î±[R+Î³V(s\n",
        "â€²\n",
        " )âˆ’V(s)]\n",
        "Where:\n",
        "ğ›¼\n",
        "Î±: Learning rate\n",
        "ğ›¾\n",
        "Î³: Discount factor\n",
        "ğ‘…\n",
        "R: Reward for the transition\n",
        "ğ‘ \n",
        "â€²\n",
        "s\n",
        "â€²\n",
        " : Next state\n",
        "Random Policy:\n",
        "\n",
        "The agent chooses actions randomly to explore the environment.\n",
        "Results:\n",
        "\n",
        "After training, the algorithm outputs the learned value of each state."
      ],
      "metadata": {
        "id": "Wit_IdVcBna3"
      }
    }
  ]
}